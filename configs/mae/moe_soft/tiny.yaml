model_target: moe_softmax.moe_model_softmax.MAE_ViT_2_T_moe
seed: 2022
batch_size: 4096
max_device_batch_size: 64
base_learning_rate: 1.5e-4
weight_decay: 0.05
mask_ratio: 0.75
total_epoch: 2000
warmup_epoch: 200
loss: l2
mixed_precision: 'fp16'
save_model_path: save_model_path/mae/moe_soft
save_model_name: moe_soft_tiny
use_aux_dataset: false
optimizer:
  target: torch.optim.AdamW
  configs:
    blr: 1.0e-4
    betas: [0.9, 0.999]
    weight_decay: 0.05





